{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "r\n",
      "t\n",
      "p\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from string import ascii_letters as eng_letters\n",
    "from string import digits\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "import pprint\n",
    "import time\n",
    "from FiniteMachineState import FSM , grep_regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import stop_words_handler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-11-10 15:15:25,926 - DEBUG]: perform system check...\n",
      "[2020-11-10 15:15:25,927 - DEBUG]: check java version...\n",
      "[2020-11-10 15:15:25,999 - DEBUG]: Your java version is 1.8 which is compatiple with Farasa \n",
      "[2020-11-10 15:15:26,000 - DEBUG]: check toolkit binaries...\n",
      "[2020-11-10 15:15:26,001 - INFO]: Dependencies seem to be satisfied..\n",
      "[2020-11-10 15:15:26,001 - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2020-11-10 15:15:26,001 - INFO]: \u001b[37minitializing [STEM] task in \u001b[32mINTERACTIVE \u001b[37mmode...\n",
      "[2020-11-10 15:15:28,586 - INFO]: task [STEM] is initialized interactively.\n",
      "[2020-11-10 15:15:28,587 - DEBUG]: perform system check...\n",
      "[2020-11-10 15:15:28,587 - DEBUG]: check java version...\n",
      "[2020-11-10 15:15:28,646 - DEBUG]: Your java version is 1.8 which is compatiple with Farasa \n",
      "[2020-11-10 15:15:28,646 - DEBUG]: check toolkit binaries...\n",
      "[2020-11-10 15:15:28,647 - INFO]: Dependencies seem to be satisfied..\n",
      "[2020-11-10 15:15:28,648 - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2020-11-10 15:15:28,649 - INFO]: \u001b[37minitializing [SEGMENT] task in \u001b[32mINTERACTIVE \u001b[37mmode...\n",
      "[2020-11-10 15:15:30,616 - INFO]: task [SEGMENT] is initialized interactively.\n"
     ]
    }
   ],
   "source": [
    "yaqutz_pd = pd.read_excel('data/structured/FullYaqut.xlsx', sheet_name='ALL')\n",
    "yaqutz_pd_types = pd.read_excel('data/structured/FullYaqut.xlsx', sheet_name='Type')\n",
    "yaqutz_pd_types = set(yaqutz_pd_types['Type_Arabic'])\n",
    "stemmer = FarasaStemmer(interactive=True)\n",
    "segmentor = FarasaSegmenter(interactive=True)\n",
    "\n",
    "s = stop_words_handler.Stopwords_Handler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Odin\\PycharmProjects\\Taqutz_information_Extraction\\stop_words_handler.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  place_pre['@voweledform'] = place_pre['@voweledform'].apply(lambda x : strip_harakat(x))\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "preps = set(s.get_all_place_pre())\n",
    "all_preps = s.unvoweled_df.copy()\n",
    "types =  yaqutz_pd_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = yaqutz_pd['PlaceName'].dropna().tolist()\n",
    "placenames = [clean_text(x,stem=False) for x in places]\n",
    "placenames = set(placenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is alot of imbiguty refarding placenames, so we remove all placename that happend to be in type list or prep list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'أرض',\n",
       " 'أطم',\n",
       " 'أمة',\n",
       " 'أودية',\n",
       " 'إقليم',\n",
       " 'الغوطة',\n",
       " 'بئر',\n",
       " 'بحر',\n",
       " 'بحيرة',\n",
       " 'بركة',\n",
       " 'بساتين',\n",
       " 'بلاد',\n",
       " 'بلد',\n",
       " 'بلدان',\n",
       " 'بلدة',\n",
       " 'بلدتان',\n",
       " 'بليد',\n",
       " 'بليدة',\n",
       " 'تل',\n",
       " 'ثغر',\n",
       " 'ثغور',\n",
       " 'ثور',\n",
       " 'جامع',\n",
       " 'جبال',\n",
       " 'جبل',\n",
       " 'جبلة',\n",
       " 'جزائر',\n",
       " 'جزيرة',\n",
       " 'جسر',\n",
       " 'جند',\n",
       " 'جيل',\n",
       " 'حبل',\n",
       " 'حد',\n",
       " 'حصن',\n",
       " 'حصون',\n",
       " 'خان',\n",
       " 'خيال',\n",
       " 'ديار',\n",
       " 'دير',\n",
       " 'رابيتان',\n",
       " 'ربض',\n",
       " 'رستاق',\n",
       " 'رمل',\n",
       " 'رية',\n",
       " 'زاب',\n",
       " 'سحل',\n",
       " 'سقيفة',\n",
       " 'سوق',\n",
       " 'شجر',\n",
       " 'شعب',\n",
       " 'صقع',\n",
       " 'صنم',\n",
       " 'ضيعة',\n",
       " 'طريق',\n",
       " 'عمل',\n",
       " 'عمود',\n",
       " 'عين',\n",
       " 'فرضة',\n",
       " 'قبيلة',\n",
       " 'قرى',\n",
       " 'قرية',\n",
       " 'قريتان',\n",
       " 'قريتين',\n",
       " 'قصبة',\n",
       " 'قصر',\n",
       " 'قصران',\n",
       " 'قلاع',\n",
       " 'قلعة',\n",
       " 'قلعتين',\n",
       " 'كرخ',\n",
       " 'كور',\n",
       " 'كورة',\n",
       " 'كورتان',\n",
       " 'لغة',\n",
       " 'ما',\n",
       " 'ماء',\n",
       " 'ماءة',\n",
       " 'مازحين',\n",
       " 'محال',\n",
       " 'محلة',\n",
       " 'مدينة',\n",
       " 'مرج',\n",
       " 'مرحلة',\n",
       " 'منارة',\n",
       " 'منخفض',\n",
       " 'منزل',\n",
       " 'مواضع',\n",
       " 'موضع',\n",
       " 'موضعان',\n",
       " 'ناحية',\n",
       " 'نبع',\n",
       " 'نهر',\n",
       " 'نهران',\n",
       " 'هضبة',\n",
       " 'واد',\n",
       " 'وادي',\n",
       " 'واديان',\n",
       " 'ولاية'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preps = set(all_preps['@unvoweledform'])\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'أرض',\n",
       " 'أطم',\n",
       " 'أمة',\n",
       " 'أودية',\n",
       " 'إقليم',\n",
       " 'الغوطة',\n",
       " 'بئر',\n",
       " 'بحر',\n",
       " 'بحيرة',\n",
       " 'بركة',\n",
       " 'بساتين',\n",
       " 'بلاد',\n",
       " 'بلد',\n",
       " 'بلدان',\n",
       " 'بلدة',\n",
       " 'بلدتان',\n",
       " 'بليد',\n",
       " 'بليدة',\n",
       " 'تل',\n",
       " 'ثغر',\n",
       " 'ثغور',\n",
       " 'ثور',\n",
       " 'جامع',\n",
       " 'جبال',\n",
       " 'جبل',\n",
       " 'جبلة',\n",
       " 'جزائر',\n",
       " 'جزيرة',\n",
       " 'جسر',\n",
       " 'جند',\n",
       " 'جيل',\n",
       " 'حبل',\n",
       " 'حد',\n",
       " 'حصن',\n",
       " 'حصون',\n",
       " 'خان',\n",
       " 'خيال',\n",
       " 'ديار',\n",
       " 'دير',\n",
       " 'رابيتان',\n",
       " 'ربض',\n",
       " 'رستاق',\n",
       " 'رمل',\n",
       " 'رية',\n",
       " 'زاب',\n",
       " 'سحل',\n",
       " 'سقيفة',\n",
       " 'سوق',\n",
       " 'شجر',\n",
       " 'شعب',\n",
       " 'صقع',\n",
       " 'صنم',\n",
       " 'ضيعة',\n",
       " 'طريق',\n",
       " 'عمل',\n",
       " 'عمود',\n",
       " 'عين',\n",
       " 'فرضة',\n",
       " 'قبيلة',\n",
       " 'قرى',\n",
       " 'قرية',\n",
       " 'قريتان',\n",
       " 'قريتين',\n",
       " 'قصبة',\n",
       " 'قصر',\n",
       " 'قصران',\n",
       " 'قلاع',\n",
       " 'قلعة',\n",
       " 'قلعتين',\n",
       " 'كرخ',\n",
       " 'كور',\n",
       " 'كورة',\n",
       " 'كورتان',\n",
       " 'لغة',\n",
       " 'ما',\n",
       " 'ماء',\n",
       " 'ماءة',\n",
       " 'مازحين',\n",
       " 'محال',\n",
       " 'محلة',\n",
       " 'مدينة',\n",
       " 'مرج',\n",
       " 'مرحلة',\n",
       " 'منارة',\n",
       " 'منخفض',\n",
       " 'منزل',\n",
       " 'مواضع',\n",
       " 'موضع',\n",
       " 'موضعان',\n",
       " 'ناحية',\n",
       " 'نبع',\n",
       " 'نهر',\n",
       " 'نهران',\n",
       " 'هضبة',\n",
       " 'واد',\n",
       " 'وادي',\n",
       " 'واديان',\n",
       " 'ولاية'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames & types \n",
    "placenames & preps\n",
    "placenames = placenames - types\n",
    "placenames = placenames - preps\n",
    "placenames= placenames - all_preps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split placenames by n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_placenames = {}\n",
    "\n",
    "for place in list(placenames):\n",
    "        ngram = len(place.split(' '))\n",
    "        if ngram in range(4):\n",
    "            ngram = str(ngram)\n",
    "            if ngram in ngrams_placenames:\n",
    "                ngrams_placenames[ngram].append(place)\n",
    "            else:\n",
    "                ngrams_placenames[ngram] = []\n",
    "                ngrams_placenames[ngram].append(place)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'هو' in placenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " example  =  \"\"\"بفتح الهمزة وسكون الألف وضم الباء الموحدة ورا : قرية من قرى دير الخوات \n",
    " ينسب إليها أبو الحسن محمد بن الحسين بن ابراهيم بن عاصم الآبر ، شيخ من\n",
    " أئمة الحدي ، له كتاب نفيس كبير في أخبار الإمام أبي عبد الله محمد بن\n",
    " إدريس الشافع ، رضي الله عن ، أجاد فيه كل الإجاد ، وكان رحل إلى مصر\n",
    " والشام والحجاز والعراق وخراسا ، روى عن أبي بكر بن خزيم ، والربيع بن\n",
    " سليمان الجيز ، وكان يعد في الحفاظ \n",
    "  روى عنه علي بن بشرى السجستان ، وذكر الفراء أنه توفي في رجب سنة \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'واسط' in ngrams_placenames['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence, hard=False,stem=True):\n",
    "    words = []\n",
    "\n",
    "    if not hard:\n",
    "        punc = '''</>~%#`÷×؛_()*&^%][ـ/\"؟'{}~¦+|!”…“–ـ'''\n",
    "        punc = punc + eng_letters\n",
    "    else:\n",
    "        english_punctuations = string.punctuation\n",
    "        arabic_punctuations = '''`÷×؛_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "        punc = english_punctuations + arabic_punctuations\n",
    "\n",
    "    for w in sentence.split():\n",
    "\n",
    "        # Cleaning Numbers\n",
    "        w = w.translate(remove_digits)\n",
    "\n",
    "        # Removing the punctuations\n",
    "        for ch in punc:\n",
    "            w = w.replace(ch, '')\n",
    "\n",
    "        if any(sep in w for sep in ['.', '،', ':']):\n",
    "            w = w[0:-1] + ' ' + w[-1]\n",
    "\n",
    "        if not re.search(r'[a-zA-Z]', w):\n",
    "            ## here we should handle ب and بال \n",
    "            if stem:\n",
    "                words.append(stemmer.stem(w))\n",
    "            else:\n",
    "                words.append(w)\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based(clean_text,stem_text,i,_dict):\n",
    "    flag = True\n",
    "    if i == len(clean_text):\n",
    "        return -1\n",
    "    if _dict:\n",
    "        if i - _dict[-1]['@index'] > 5 :\n",
    "            return i\n",
    "    token = stem_text[i]\n",
    "    clean_token = clean_text[i]\n",
    "    for ix in range(4,-1,-1):\n",
    "        if i < len(clean_text) + ix :\n",
    "            n_gram_clean_token = ' '.join(clean_text[i:i+ix])\n",
    "\n",
    "            if n_gram_clean_token in placenames:\n",
    "\n",
    "                _dict.append({'placename':n_gram_clean_token,'@index':i})\n",
    "                i=i+ix\n",
    "                flag = False\n",
    "                \n",
    "    \n",
    "    if flag:\n",
    "        if token in preps:\n",
    "\n",
    "            _dict.append({'prep':clean_text[i],'@index':i})\n",
    "\n",
    "        elif token in types:\n",
    "            _dict.append({'type':clean_text[i],'@index':i})\n",
    "\n",
    "\n",
    "    return rule_based(clean_text,stem_text,i+1,_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based2(clean_text,stem_text):\n",
    "    rules=[]\n",
    "    _dict=[]\n",
    "    for i in range(len(clean_text)):\n",
    "        \n",
    "        \n",
    "        if _dict:\n",
    "            if i - _dict[-1]['@index'] > 5 :\n",
    "                if is_rule(_dict):\n",
    "                    rules.append(_dict)\n",
    "                    _dict=[]\n",
    "                    \n",
    "                    \n",
    "        flag = True\n",
    "        token = stem_text[i]\n",
    "        clean_token = clean_text[i]\n",
    "        for ix in range(4,0,-1):\n",
    "            if i < len(clean_text) + ix :\n",
    "                n_gram_clean_token = ' '.join(clean_text[i:i+ix])\n",
    "\n",
    "                if n_gram_clean_token in placenames:\n",
    "\n",
    "                    _dict.append({'placename':n_gram_clean_token,'@index':i})\n",
    "                    i=i+ix\n",
    "                    flag = False\n",
    "\n",
    "\n",
    "        if flag:\n",
    "            if token in preps:\n",
    "\n",
    "                _dict.append({'prep':clean_text[i],'@index':i})\n",
    "\n",
    "            elif token in types:\n",
    "                _dict.append({'type':clean_text[i],'@index':i})\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based3( text ,_filter=True ):\n",
    "\n",
    "    text = clean_text(text,stem=False)\n",
    "\n",
    "    entities = []\n",
    "    _dict = []\n",
    "    pattern = re.compile(r\" ، | : | . \")\n",
    "    for sentence in pattern.split(text):\n",
    "\n",
    "        _dict = []\n",
    "        \n",
    "        \n",
    "        tokens = sentence.split()\n",
    "        stem_tokens = stemmer.stem(sentence).split()\n",
    "        \n",
    "        for index in range (len(tokens)):\n",
    "\n",
    "\n",
    "            flag = True\n",
    "            for ix in range(3,0,-1):\n",
    "                \n",
    "                \n",
    "                \n",
    "                if index + ix < len(sentence.split()) + 1  :\n",
    "                    \n",
    "                    n_gram_clean_token = ' '.join(tokens[index:index+ix])\n",
    "\n",
    "                    if n_gram_clean_token in ngrams_placenames[str(ix)]:\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        _dict.append({'placename':n_gram_clean_token,'@index':index})\n",
    "                        index+=ix\n",
    "                        flag = False\n",
    "                        #check for AL- or W- \n",
    "                    else: \n",
    "                        if drop_prefix(n_gram_clean_token) in ngrams_placenames[str(ix)]:\n",
    "\n",
    "                        \n",
    "                            _dict.append({'placename':n_gram_clean_token,'@index':index})\n",
    "                            index+=ix\n",
    "                            flag = False\n",
    "\n",
    "                            \n",
    "            if flag :\n",
    "                \n",
    "                \n",
    "                if stem_tokens[index] in types:\n",
    "\n",
    "\n",
    "                    \n",
    "                    _dict.append({'type':stem_tokens[index],'@index':index})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if stem_tokens[index] in preps:\n",
    "                    # TODO:\n",
    "\n",
    "                    \n",
    "                    _dict.append({'prep':tokens[index],'@index':index})\n",
    "\n",
    "               \n",
    "                \n",
    "        if _dict:\n",
    "\n",
    "            if len(_dict) > 3:\n",
    "                \n",
    "                if _filter and parser_and_grep(_dict):\n",
    "\n",
    "                    entities.append(_dict) \n",
    "                else:\n",
    "                    entities.append(_dict) \n",
    "#         print(sentence,_dict)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-251c6fcf2e67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtick\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myaqutz_pd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mrule_based3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtack\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\odin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4043\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4045\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4047\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-251c6fcf2e67>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtick\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myaqutz_pd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mrule_based3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtack\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-e577b3c746a9>\u001b[0m in \u001b[0;36mrule_based3\u001b[1;34m(text, _filter)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mstem_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\odin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\farasa\\stemmer.py\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\odin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\farasa\\__base.py\u001b[0m in \u001b[0;36m_do_task\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0mstrip_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__interactive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_task_interactive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrip_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_task_standalone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrip_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\odin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\farasa\\__base.py\u001b[0m in \u001b[0;36m_do_task_interactive\u001b[1;34m(self, strip_text)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mnewlined_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[0mbyted_newlined_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewlined_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_task_interactive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyted_newlined_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\odin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\farasa\\__base.py\u001b[0m in \u001b[0;36m_run_task_interactive\u001b[1;34m(self, btext)\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__task_proc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__task_proc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__task_proc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "rules = yaqutz_pd['Description'].apply(lambda x : rule_based3(x))\n",
    "tack = time.time()\n",
    "print(tack-tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rules_true = rules[rules.map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_false = rules[~(rules.map(lambda d: len(d)) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flase_subset = yaqutz_pd[~(rules.map(lambda d: len(d)) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flase_subset['rule'] = flase_subset['Description'].apply(lambda x : rule_based3(x,_filter=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flase_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = (clean_text(flase_subset.Description.iloc[0],stem=False))\n",
    "print(x)\n",
    "print(parser_and_grep( rule_based3('قرية من قرى سجستان')[0]))\n",
    "# flase_subset.rule.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ex = clean_text(example,stem=False)\n",
    "clean_ex = ' '.join(clean_ex.split(\"\\n\"))\n",
    "stem_ex = clean_text(example)\n",
    "stem_ex = ' '.join(stem_ex.split(\"\\n\"))\n",
    "clean_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dict = []\n",
    "result= rule_based(clean_ex.split(),stem_ex.split(),0,_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dict = []\n",
    "result= rule_based2(clean_ex.split(),stem_ex.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in data:\n",
    "    print(i,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  input : list of dict \n",
    "[{'type': 'قرية', '@index': 9},\n",
    " {'prep': 'من', '@index': 10},\n",
    " {'type': 'قرى', '@index': 11},\n",
    " {'placename': 'حزم نمير', '@index': 12}]\n",
    " \n",
    "    return: True if the rule is valid \n",
    " '''\n",
    "\n",
    "\n",
    "def parser_and_grep(_dict):\n",
    "    parser = ''\n",
    "    evaluator = FSM()\n",
    "    for ele in _dict :\n",
    "        if 'type' in ele:\n",
    "            parser+='t'\n",
    "        elif 'prep' in ele:\n",
    "            parser += 'r'\n",
    "        elif 'placename' in ele:\n",
    "            parser += 'p'\n",
    "\n",
    "\n",
    "    for ch in parser:\n",
    "\n",
    "        evaluator.send(ch)\n",
    "    return evaluator.does_match()   \n",
    "\n",
    "parser_and_grep([{'type': 'بليد', '@index': 0},\n",
    "  {'placename': 'ساحل', '@index': 2},\n",
    "  {'type': 'بحر', '@index': 3},\n",
    "  {'placename': 'طبرستان', '@index': 4},\n",
    "  {'prep': 'بين', '@index': 5},\n",
    "  {'prep': 'بين', '@index': 6},\n",
    "  {'placename': 'جرجان', '@index': 7}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'جمع'\n",
    "\n",
    "def drop_prefix(phrase):\n",
    "\n",
    "    \n",
    "    tokens = phrase.split()\n",
    "    if(len(tokens) > 0):\n",
    "        token = tokens[0]\n",
    "        segments = segmentor.segment(token).split('+')\n",
    "        if segments[0] in ['و','ال','أل']:\n",
    "            segments = segments[1:]\n",
    "            tokens[0] = ''.join(segments)\n",
    "        return ' '.join(tokens)\n",
    "    return \n",
    "drop_prefix(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(2,-1,-1):\n",
    "    print(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1','2','3','4']\n",
    "for i in range(4):\n",
    "    for ix in range(3,0,-1):\n",
    "        if(i+ix < 5 ):\n",
    "            print(x[i:i+ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 'tpptprrp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
