{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "r\n",
      "t\n",
      "p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-11-29 09:43:54,669 - DEBUG]: perform system check...\n",
      "[2020-11-29 09:43:54,670 - DEBUG]: check java version...\n",
      "[2020-11-29 09:43:54,933 - DEBUG]: Your java version is 1.8 which is compatiple with Farasa \n",
      "[2020-11-29 09:43:54,933 - DEBUG]: check toolkit binaries...\n",
      "[2020-11-29 09:43:54,934 - INFO]: Dependencies seem to be satisfied..\n",
      "[2020-11-29 09:43:54,935 - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2020-11-29 09:43:54,935 - INFO]: \u001b[37minitializing [STEM] task in \u001b[32mINTERACTIVE \u001b[37mmode...\n",
      "[2020-11-29 09:43:57,250 - INFO]: task [STEM] is initialized interactively.\n",
      "[2020-11-29 09:43:57,251 - DEBUG]: perform system check...\n",
      "[2020-11-29 09:43:57,251 - DEBUG]: check java version...\n",
      "[2020-11-29 09:43:57,319 - DEBUG]: Your java version is 1.8 which is compatiple with Farasa \n",
      "[2020-11-29 09:43:57,320 - DEBUG]: check toolkit binaries...\n",
      "[2020-11-29 09:43:57,321 - INFO]: Dependencies seem to be satisfied..\n",
      "[2020-11-29 09:43:57,321 - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2020-11-29 09:43:57,322 - INFO]: \u001b[37minitializing [SEGMENT] task in \u001b[32mINTERACTIVE \u001b[37mmode...\n",
      "[2020-11-29 09:43:59,595 - INFO]: task [SEGMENT] is initialized interactively.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from string import ascii_letters as eng_letters\n",
    "from string import digits\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "# from farasa.stemmer import FarasaStemmer\n",
    "# from farasa.segmenter import FarasaSegmenter\n",
    "import pprint\n",
    "import time\n",
    "from FiniteMachineState import FSM , grep_regex\n",
    "import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  'df_biwords',\n",
       "  \"import re\\nimport pandas as pd\\nfrom string import ascii_letters as eng_letters\\nfrom string import digits\\nremove_digits = str.maketrans('', '', digits)\\n# from farasa.stemmer import FarasaStemmer\\n# from farasa.segmenter import FarasaSegmenter\\nimport pprint\\nimport time\\nfrom FiniteMachineState import FSM , grep_regex\\nimport Tools\",\n",
       "  'from pprint import pprint as print',\n",
       "  'locals()'],\n",
       " '_oh': {},\n",
       " '_dh': ['C:\\\\Users\\\\Odin\\\\PycharmProjects\\\\Chalifat-Mapping-Based-on-Al-Yaqut'],\n",
       " 'In': ['',\n",
       "  'df_biwords',\n",
       "  \"import re\\nimport pandas as pd\\nfrom string import ascii_letters as eng_letters\\nfrom string import digits\\nremove_digits = str.maketrans('', '', digits)\\n# from farasa.stemmer import FarasaStemmer\\n# from farasa.segmenter import FarasaSegmenter\\nimport pprint\\nimport time\\nfrom FiniteMachineState import FSM , grep_regex\\nimport Tools\",\n",
       "  'from pprint import pprint as print',\n",
       "  'locals()'],\n",
       " 'Out': {},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x000002C358B28358>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x2c358c0aba8>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x2c358c0aba8>,\n",
       " '_': '',\n",
       " '__': '',\n",
       " '___': '',\n",
       " '_i': 'from pprint import pprint as print',\n",
       " '_ii': \"import re\\nimport pandas as pd\\nfrom string import ascii_letters as eng_letters\\nfrom string import digits\\nremove_digits = str.maketrans('', '', digits)\\n# from farasa.stemmer import FarasaStemmer\\n# from farasa.segmenter import FarasaSegmenter\\nimport pprint\\nimport time\\nfrom FiniteMachineState import FSM , grep_regex\\nimport Tools\",\n",
       " '_iii': 'df_biwords',\n",
       " '_i1': 'df_biwords',\n",
       " '_i2': \"import re\\nimport pandas as pd\\nfrom string import ascii_letters as eng_letters\\nfrom string import digits\\nremove_digits = str.maketrans('', '', digits)\\n# from farasa.stemmer import FarasaStemmer\\n# from farasa.segmenter import FarasaSegmenter\\nimport pprint\\nimport time\\nfrom FiniteMachineState import FSM , grep_regex\\nimport Tools\",\n",
       " 're': <module 're' from 'c:\\\\users\\\\odin\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\re.py'>,\n",
       " 'pd': <module 'pandas' from 'c:\\\\users\\\\odin\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\site-packages\\\\pandas\\\\__init__.py'>,\n",
       " 'eng_letters': 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',\n",
       " 'digits': '0123456789',\n",
       " 'remove_digits': {48: None,\n",
       "  49: None,\n",
       "  50: None,\n",
       "  51: None,\n",
       "  52: None,\n",
       "  53: None,\n",
       "  54: None,\n",
       "  55: None,\n",
       "  56: None,\n",
       "  57: None},\n",
       " 'pprint': <module 'pprint' from 'c:\\\\users\\\\odin\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\pprint.py'>,\n",
       " 'time': <module 'time' (built-in)>,\n",
       " 'FSM': FiniteMachineState.FSM,\n",
       " 'grep_regex': <function FiniteMachineState.grep_regex(text)>,\n",
       " 'Tools': <module 'Tools' from 'C:\\\\Users\\\\Odin\\\\PycharmProjects\\\\Chalifat-Mapping-Based-on-Al-Yaqut\\\\Tools.py'>,\n",
       " '_i3': 'from pprint import pprint as print',\n",
       " 'print': <function pprint.pprint(object, stream=None, indent=1, width=80, depth=None, *, compact=False)>,\n",
       " '_i4': 'locals()'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import stop_words_handler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaqutz_pd = pd.read_excel('data/structured/FullYaqut.xlsx', sheet_name='ALL')\n",
    "yaqutz_pd_types = pd.read_excel('data/structured/FullYaqut.xlsx', sheet_name='Type')\n",
    "yaqutz_pd_types = set(yaqutz_pd_types['Type_Arabic'])\n",
    "s = stop_words_handler.Stopwords_Handler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Odin\\PycharmProjects\\Chalifat-Mapping-Based-on-Al-Yaqut\\stop_words_handler.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  place_pre['@voweledform'] = place_pre['@voweledform'].apply(lambda x : strip_harakat(x))\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "preps = set(s.get_all_place_pre())\n",
    "all_preps = s.unvoweled_df.copy()\n",
    "all_preps = set(all_preps ['@unvoweledform'])\n",
    "types =  yaqutz_pd_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "\n",
    "# Initialize logging.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)  # DEBUG # INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_placename = pd.read_csv('data/util/exclue_placename.csv')\n",
    "ex_placename=set(ex_placename.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "infered_placenames = pd.read_csv('data/util/Placenames_inferd_from_text.csv')\n",
    "infered_placenames = infered_placenames.name\n",
    "infered_placenames = infered_placenames.drop_duplicates()\n",
    "infered_placenames = set(infered_placenames)\n",
    "infered_placenames = infered_placenames - types\n",
    "infered_placenames = infered_placenames - preps\n",
    "infered_placenames= infered_placenames - all_preps\n",
    "infered_placenames = infered_placenames - ex_placename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'أخر',\n",
       " 'أرمينية',\n",
       " 'أسد',\n",
       " 'أسلم',\n",
       " 'أشروسنة',\n",
       " 'إرمينية',\n",
       " 'إيلاق',\n",
       " 'ابن ليون',\n",
       " 'افريقية',\n",
       " 'الأحقاف',\n",
       " 'الأردن',\n",
       " 'الأرمن',\n",
       " 'الأزد',\n",
       " 'الأشعريين',\n",
       " 'الأفرنج',\n",
       " 'الأندلس',\n",
       " 'الإسلام',\n",
       " 'البجاه',\n",
       " 'البجة',\n",
       " 'البربر',\n",
       " 'التبت',\n",
       " 'الترك',\n",
       " 'الثغر',\n",
       " 'الثغور',\n",
       " 'الجبل',\n",
       " 'الجريد',\n",
       " 'الجزيرة',\n",
       " 'الحبشة',\n",
       " 'الحزن',\n",
       " 'الختل',\n",
       " 'الخزر',\n",
       " 'الداور',\n",
       " 'الديلم',\n",
       " 'الرباب',\n",
       " 'الروم',\n",
       " 'الزنج',\n",
       " 'الساحل',\n",
       " 'السقوياسيس',\n",
       " 'السند',\n",
       " 'السواحل',\n",
       " 'السودان',\n",
       " 'الشاش',\n",
       " 'الشام',\n",
       " 'الشحر',\n",
       " 'الصرود',\n",
       " 'الصقالبة',\n",
       " 'الصين',\n",
       " 'الضباب',\n",
       " 'الطور',\n",
       " 'العجلان',\n",
       " 'العجم',\n",
       " 'العراق',\n",
       " 'العرب',\n",
       " 'العماليق',\n",
       " 'العواصم',\n",
       " 'الغرب',\n",
       " 'الغور',\n",
       " 'الفرس',\n",
       " 'الفرنج',\n",
       " 'الفهلويين',\n",
       " 'القمر',\n",
       " 'القين',\n",
       " 'الكفر',\n",
       " 'الله',\n",
       " 'المسلمين',\n",
       " 'المصامدة',\n",
       " 'المصيصة',\n",
       " 'المعافر',\n",
       " 'المعجم',\n",
       " 'المغرب',\n",
       " 'الملثمين',\n",
       " 'النصرانية',\n",
       " 'النوبة',\n",
       " 'الهكارية',\n",
       " 'الهند',\n",
       " 'الهياطلة',\n",
       " 'اليستعور',\n",
       " 'اليمامة',\n",
       " 'اليمن',\n",
       " 'اليونان',\n",
       " 'باهلة',\n",
       " 'بخارى',\n",
       " 'بربر',\n",
       " 'بكر',\n",
       " 'بلحرث',\n",
       " 'بلعنبر',\n",
       " 'بلغار',\n",
       " 'بني أسد',\n",
       " 'بني الحارث',\n",
       " 'بني العجلان',\n",
       " 'بني العنبر',\n",
       " 'بني تغلب',\n",
       " 'بني تميم',\n",
       " 'بني جذيمة',\n",
       " 'بني جعفر',\n",
       " 'بني حبيش',\n",
       " 'بني حنظلة',\n",
       " 'بني ذبيان',\n",
       " 'بني زبيد',\n",
       " 'بني سعد',\n",
       " 'بني سلول',\n",
       " 'بني سليم',\n",
       " 'بني شيبان',\n",
       " 'بني ضبة',\n",
       " 'بني ضمرة',\n",
       " 'بني عامر',\n",
       " 'بني عبس',\n",
       " 'بني عذرة',\n",
       " 'بني عقيل',\n",
       " 'بني عمرو',\n",
       " 'بني عمير',\n",
       " 'بني عوف',\n",
       " 'بني فزارة',\n",
       " 'بني قشير',\n",
       " 'بني كعب',\n",
       " 'بني كلاب',\n",
       " 'بني كنانة',\n",
       " 'بني لحيان',\n",
       " 'بني ليون',\n",
       " 'بني مازن',\n",
       " 'بني مرة',\n",
       " 'بني مزينة',\n",
       " 'بني نصر',\n",
       " 'بني نمير',\n",
       " 'بني هذيل',\n",
       " 'بني يربوع',\n",
       " 'تركستان',\n",
       " 'تغلب',\n",
       " 'تمم',\n",
       " 'تميم',\n",
       " 'تهامة',\n",
       " 'تيم',\n",
       " 'جذام',\n",
       " 'جرزان',\n",
       " 'جعفر',\n",
       " 'جليقية',\n",
       " 'جهينة',\n",
       " 'جيلان',\n",
       " 'حلوان',\n",
       " 'خاقان',\n",
       " 'خثعم',\n",
       " 'خراسان',\n",
       " 'خزاعة',\n",
       " 'خوزستان',\n",
       " 'خولان',\n",
       " 'دوس',\n",
       " 'زويلة',\n",
       " 'سعد',\n",
       " 'سلوقية',\n",
       " 'سليم',\n",
       " 'سميساط',\n",
       " 'شتى',\n",
       " 'ضبة',\n",
       " 'طبرستان',\n",
       " 'طيء',\n",
       " 'عاد',\n",
       " 'عامر',\n",
       " 'عبس',\n",
       " 'عدوان',\n",
       " 'عذرة',\n",
       " 'عقيل',\n",
       " 'عكل',\n",
       " 'عمرو',\n",
       " 'غريبة',\n",
       " 'غطفان',\n",
       " 'غفار',\n",
       " 'غني',\n",
       " 'غيرها',\n",
       " 'غيرهم',\n",
       " 'فارس',\n",
       " 'فرسان',\n",
       " 'فرغانة',\n",
       " 'فزارة',\n",
       " 'فهم',\n",
       " 'فيها',\n",
       " 'قردى',\n",
       " 'قشير',\n",
       " 'قضاعة',\n",
       " 'قوم',\n",
       " 'قومه',\n",
       " 'قيس',\n",
       " 'كانت',\n",
       " 'كتامة',\n",
       " 'كثيرة',\n",
       " 'كرمان',\n",
       " 'كلاب',\n",
       " 'كلب',\n",
       " 'كنانة',\n",
       " 'لمتونة',\n",
       " 'متفرقة',\n",
       " 'مثل',\n",
       " 'محارب',\n",
       " 'مذحج',\n",
       " 'مراد',\n",
       " 'مرة',\n",
       " 'مزينة',\n",
       " 'مشتكهر',\n",
       " 'مصر',\n",
       " 'مهرة',\n",
       " 'نخلة بالصي',\n",
       " 'نخلة في',\n",
       " 'نمر',\n",
       " 'نمير',\n",
       " 'هذيل',\n",
       " 'همدان',\n",
       " 'هوارة',\n",
       " 'هوازن',\n",
       " 'واسعة',\n",
       " 'وخاب'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infered_placenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'شام' to placenames\n",
    "# add thurya placenames\n",
    "places = yaqutz_pd['PlaceName'].dropna().tolist()\n",
    "placenames = [clean_text(x,stem=False) for x in places]\n",
    "placenames = set(placenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames.add('شام')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom placenames, type , prep Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is alot of imbiguty refarding placenames, so we remove all placename that happend to be in type list or prep list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_numbers =set(['نصف',\n",
    " 'واحد',\n",
    " 'اثنان',\n",
    " 'ثلاثة',\n",
    " 'أربعة',\n",
    " 'خمسة',\n",
    " 'ستة',\n",
    " 'سبعة',\n",
    " 'ثمانية',\n",
    " 'تسعة',\n",
    " 'عشرة'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ثمانية', 'خمسة', 'واحد'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_numbers & placenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames = placenames | infered_placenames\n",
    "placenames = placenames - types\n",
    "placenames = placenames - preps\n",
    "placenames= placenames - all_preps\n",
    "placenames = placenames - ex_placename\n",
    "placenames = placenames - arabic_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_numbers & placenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'بلقين' in placenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12338"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(placenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps.remove('بين')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'جند' in types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "types.add('عواصم')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'عند' in preps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split placenames by n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_placenames = {}\n",
    "\n",
    "for place in list(placenames):\n",
    "        ngram = len(place.split(' '))\n",
    "        if ngram in range(4):\n",
    "            ngram = str(ngram)\n",
    "            if ngram in ngrams_placenames:\n",
    "                ngrams_placenames[ngram].append(place)\n",
    "            else:\n",
    "                ngrams_placenames[ngram] = []\n",
    "                ngrams_placenames[ngram].append(place)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'بصرة' in ngrams_placenames['1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'فتح همزة سكون ألف ضم باء موحد ورا : قرية من قرية دير خو نسب إلى أبو حسن محمد ابن حسين ابن إبراهيم ابن عاصم آبر ، شيخ من إمام حدي ، ل كتاب نفيس كبير في خبر إمام أب عبد الله محمد ابن إدريس شافع ، رضي الله عن ، أجاد في كل إجاد ، كان رحل إلى مصر شام حجاز عراق خراس ، روى عن أب بكر ابن خزيم ، ربيع ابن سليمان جيز ، كان عد في حفاظ روى عن علي ابن بشرة سجستان ، ذكر فراء أن توفى في رجب سنة'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " example  =  \"\"\"بفتح الهمزة وسكون الألف وضم الباء الموحدة ورا : قرية من قرى دير الخوات \n",
    " ينسب إليها أبو الحسن محمد بن الحسين بن ابراهيم بن عاصم الآبر ، شيخ من\n",
    " أئمة الحدي ، له كتاب نفيس كبير في أخبار الإمام أبي عبد الله محمد بن\n",
    " إدريس الشافع ، رضي الله عن ، أجاد فيه كل الإجاد ، وكان رحل إلى مصر\n",
    " والشام والحجاز والعراق وخراسا ، روى عن أبي بكر بن خزيم ، والربيع بن\n",
    " سليمان الجيز ، وكان يعد في الحفاظ \n",
    "  روى عنه علي بن بشرى السجستان ، وذكر الفراء أنه توفي في رجب سنة \"\"\"\n",
    "clean_text(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo Handle 'ب ال '\n",
    "\n",
    "\n",
    "def rule_based4(X,_filter=True,debug=False ):\n",
    "    placename ,text = X[0],X[1]\n",
    "    text = clean_text(text,stem=False)\n",
    "\n",
    "    entities = []\n",
    "    pre_word = False\n",
    "    \n",
    "    pattern = re.compile(r\" ، | : | . \")\n",
    "    for sentence in pattern.split(text):\n",
    "\n",
    "        _dict = []\n",
    "        \n",
    "        if debug:\n",
    "            print('Sentence = ' + sentence)\n",
    "        \n",
    "        tokens = sentence.split()\n",
    "        stem_tokens = stemmer.stem(sentence).split()\n",
    "        \n",
    "        for index in range (len(tokens)):\n",
    "\n",
    "\n",
    "            flag = True\n",
    "            #                               PlaceName\n",
    "            for ix in range(3,0,-1):\n",
    "                \n",
    "                \n",
    "                \n",
    "                if index + ix < len(sentence.split()) + 1  :\n",
    "                    \n",
    "                    n_gram_clean_token = ' '.join(tokens[index:index+ix])\n",
    "\n",
    "                    if n_gram_clean_token in ngrams_placenames[str(ix)] and not pre_word and n_gram_clean_token != placename:\n",
    "                        if not pre_word:\n",
    "                            _dict.append({'placename':n_gram_clean_token,'@index':index})\n",
    "                            index+=ix\n",
    "                            flag = False\n",
    "                        #check for AL- or W- \n",
    "                        \n",
    "                    else: \n",
    "\n",
    "                        stemmed_placename , segmes = drop_prefix(n_gram_clean_token)[0] , drop_prefix(n_gram_clean_token)[1] \n",
    "\n",
    "                        if stemmed_placename in ngrams_placenames[str(ix)] and not pre_word and stemmed_placename != placename :\n",
    "                        \n",
    "                            if(segmes[0] in ['ب','بال']):\n",
    "\n",
    "\n",
    "                                _dict.append({'prep':segmes[0],'@index':str(index - 0.5)})\n",
    "                                _dict.append({'placename':stemmed_placename,'@index':index})\n",
    "                                \n",
    "                                if debug:\n",
    "                                    print(_dict)\n",
    "                                index+=ix\n",
    "                                flag = False\n",
    "                            else:\n",
    "                                _dict.append({'placename':stemmed_placename,'@index':index})\n",
    "                                index+=ix\n",
    "                                flag = False                                   \n",
    "\n",
    "                                    \n",
    "\n",
    "                            \n",
    "            if flag :\n",
    "                \n",
    "                _pre_drop = drop_prefix(tokens[index])[0]\n",
    "                if tokens[index] in types:\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    _dict.append({'type':stem_tokens[index],'@index':index})\n",
    "                    pre_word = False\n",
    "                    \n",
    "                elif _pre_drop in types:\n",
    "                    \n",
    "  \n",
    "                    \n",
    "                    _dict.append({'type': _pre_drop,'@index':index})\n",
    "                    pre_word = False\n",
    "\n",
    "\n",
    "        \n",
    "                if stem_tokens[index] in preps:\n",
    "                    # TODO:\n",
    "                    \n",
    "                    \n",
    "                    _dict.append({'prep':tokens[index],'@index':index})\n",
    "                    \n",
    "                elif stem_tokens[index] == 'بين':\n",
    "                    pre_word = True\n",
    "                else:\n",
    "                    pre_word = False\n",
    "\n",
    "               \n",
    "        if _dict:\n",
    "#             print(_dict)\n",
    "#             print(parser_and_grep(_dict))\n",
    "            if len(_dict) >= 3:\n",
    "                    \n",
    "                if _filter :\n",
    "                    \n",
    "                    if parser_and_grep(_dict):\n",
    "                        \n",
    "                        entities.append(_dict) \n",
    "                else:\n",
    "                    if not parser_and_grep(_dict):\n",
    "                        entities.append(_dict) \n",
    "#         print(sentence,_dict)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Sentence = ر'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'قرية قرب حلب بالعواصم في لحف جبل لبنان قديمة'\n",
    "rule_based4(text,debug=True,_filter=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['موضع بالدهناء', ['موضع']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_prefix('موضع بالدهناء')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_prefix(phrase):\n",
    "    tokens = phrase.split()\n",
    "    if (len(tokens) > 0):\n",
    "        token = tokens[0]\n",
    "        segments = segmentor.segment(token).split('+')\n",
    "        word= token\n",
    "        for t in segments:\n",
    "            if len(t) > 2 :\n",
    "                tokens[0] = t\n",
    "        return [' '.join(tokens),segments[0]]\n",
    "                \n",
    "                \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo Handle 'ب ال '\n",
    "\n",
    "\n",
    "def rule_based3( text ,_filter=True,debug=False ):\n",
    "\n",
    "    text = clean_text(text,stem=False)\n",
    "\n",
    "    entities = []\n",
    "    pre_word = False\n",
    "    \n",
    "    pattern = re.compile(r\" ، | : | . \")\n",
    "    for sentence in pattern.split(text):\n",
    "\n",
    "        _dict = []\n",
    "        \n",
    "#         if debug:\n",
    "#             print('Sentence = ' + sentence)\n",
    "        \n",
    "        tokens = sentence.split()\n",
    "        stem_tokens = stemmer.stem(sentence).split()\n",
    "        \n",
    "        for index in range (len(tokens)):\n",
    "\n",
    "\n",
    "            flag = True\n",
    "            for ix in range(3,0,-1):\n",
    "                \n",
    "                \n",
    "                \n",
    "                if index + ix < len(sentence.split()) + 1  :\n",
    "                    \n",
    "                    n_gram_clean_token = ' '.join(tokens[index:index+ix])\n",
    "#                     if debug:\n",
    "#                         print('n_gram = ' + n_gram_clean_token)\n",
    "                        \n",
    "                        \n",
    "                    if n_gram_clean_token in ngrams_placenames[str(ix)]:\n",
    "                        if not pre_word:\n",
    "                            _dict.append({'placename':n_gram_clean_token,'@index':index})\n",
    "                            index+=ix\n",
    "                            flag = False\n",
    "                        #check for AL- or W- \n",
    "                        \n",
    "                    else: \n",
    "#                         if debug:\n",
    "#                             print('no prefix = ' + drop_prefix(n_gram_clean_token) + ' ' +str(ix ) )\n",
    "#                             print( drop_prefix(n_gram_clean_token) in ngrams_placenames[str(ix)] )\n",
    "\n",
    "                        if drop_prefix(n_gram_clean_token) in ngrams_placenames[str(ix)]:\n",
    "                            \n",
    "                            \n",
    "\n",
    "                            if not pre_word:\n",
    "                                _dict.append({'placename':n_gram_clean_token,'@index':index})\n",
    "                                index+=ix\n",
    "                                flag = False\n",
    "\n",
    "                            \n",
    "            if flag :\n",
    "                \n",
    "                _pre_drop = drop_prefix(tokens[index])\n",
    "                if tokens[index] in types:\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    _dict.append({'type':stem_tokens[index],'@index':index})\n",
    "                    pre_word = False\n",
    "                    \n",
    "                elif _pre_drop in types:\n",
    "                    \n",
    "  \n",
    "                    \n",
    "                    _dict.append({'type': _pre_drop,'@index':index})\n",
    "                    pre_word = False\n",
    "\n",
    "\n",
    "        \n",
    "                if stem_tokens[index] in preps:\n",
    "                    # TODO:\n",
    "                    \n",
    "                    \n",
    "                    _dict.append({'prep':tokens[index],'@index':index})\n",
    "                    \n",
    "                elif stem_tokens[index] == 'بين':\n",
    "                    pre_word = True\n",
    "                else:\n",
    "                    pre_word = False\n",
    "\n",
    "        \n",
    "        if _dict:\n",
    "\n",
    "            if len(_dict) >= 3:\n",
    "                \n",
    "                if _filter :\n",
    "                    \n",
    "                    if parser_and_grep(_dict):\n",
    "\n",
    "                        entities.append(_dict) \n",
    "                else:\n",
    "                    entities.append(_dict) \n",
    "        print(_dict)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rules=yaqutz_pd[['PlaceName','Description']]\n",
    "rules['rule'] = rules.apply(lambda x : rule_based5(x),axis=1)\n",
    "rules_true = rules[rules['rule'].map(lambda d: len(d)) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rules_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d3027da88191>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrules_true\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rules_true' is not defined"
     ]
    }
   ],
   "source": [
    "rules_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(rules_true.iloc[i].PlaceName)\n",
    "print(rules_true.iloc[i].Description)\n",
    "print(rules_true.iloc[i].rule)\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rules_true.iloc[1000].PlaceName)\n",
    "print(rules_true.iloc[1000].Description)\n",
    "print(rules_true.iloc[1000].rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rules_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_true.iloc[0].rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rule(X):\n",
    "    _list=[]\n",
    "    for rule in X:\n",
    "\n",
    "        string = [list(i.values())[0] for i in rule]\n",
    "        _list.append(' '.join(string))\n",
    "\n",
    "    return ' | '.join(_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rules_true['rule_text'] = rules_true.rule.apply(lambda x : parse_rule(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_true['id'] = rules_true.index\n",
    "rules_true[['id','PlaceName','rule','rule_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import MariadbConnector as mdb\n",
    "\n",
    "importlib.reload(mdb)\n",
    "mdb.Connector().update_rule_table(rules_true[['id','PlaceName','rule','rule_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_false =rules[~(rules['rule'].map(lambda d: len(d)) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_true['PlaceName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Save Rules To Excel\n",
    "\n",
    "\n",
    "df= pd.DataFrame()\n",
    "for rule in rules_true.values : \n",
    "   \n",
    "    r = rule[1][0] \n",
    "    print(r)\n",
    "    i =1\n",
    "    _dict={'PlaceName' : rule[0]}\n",
    "    for ele in r :\n",
    "        \n",
    "        if 'placename' in ele:\n",
    "            keyname = 'placename'+str(i)\n",
    "            \n",
    "            _dict[keyname] = ele['placename']\n",
    "            \n",
    "            i+=1\n",
    "    print(_dict)\n",
    "    df = df.append(_dict,ignore_index=True)\n",
    "df = df.fillna('')       \n",
    "df.to_excel('rules.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1306 Valid rules extracted , need to  adjust the FSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rules_false ['unFilterd_rule'] = rules_false[['PlaceName','Description']].apply(lambda x : rule_based4(x,_filter=False),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(rules_false.iloc[j].PlaceName)\n",
    "print(rules_false.iloc[j].Description)\n",
    "# print(rules_false.iloc[j].unFilterd_rule)\n",
    "print(rule_based5([rules_false.iloc[j].PlaceName,rules_false.iloc[j].Description]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo Handle 'ب ال '\n",
    "\n",
    "\n",
    "def rule_based5(X,_filter=True,debug=False ):\n",
    "    placename ,text = X[0],X[1]\n",
    "    text = clean_text(text,stem=False)\n",
    "    skip_index = 0\n",
    "    entities = []\n",
    "    pre_word = False\n",
    "    \n",
    "    pattern = re.compile(r\" ، | : | . \")\n",
    "    \n",
    "    \n",
    "    for sentence in pattern.split(text):\n",
    "\n",
    "        _dict = []\n",
    "        \n",
    "        if debug:\n",
    "            print('Sentence = ' + sentence)\n",
    "        \n",
    "        tokens = sentence.split()\n",
    "        stem_tokens = stemmer.stem(sentence).split()\n",
    "        \n",
    "        for index in range (len(tokens)):\n",
    "            if skip_index > 1 :\n",
    "                skip_index-=1\n",
    "                \n",
    "                continue\n",
    "\n",
    "            flag = True\n",
    "            #                               PlaceName\n",
    "            for ix in range(3,0,-1):\n",
    "                \n",
    "                \n",
    "                \n",
    "                if index + ix < len(sentence.split()) + 1  :\n",
    "                    \n",
    "                    n_gram_clean_token = ' '.join(tokens[index:index+ix])\n",
    "\n",
    "                    if n_gram_clean_token in ngrams_placenames[str(ix)] and not pre_word and n_gram_clean_token != placename:\n",
    "                        if not pre_word:\n",
    "                            _dict.append({'placename':n_gram_clean_token,'@index':index})\n",
    "                            skip_index=ix\n",
    "                            flag = False\n",
    "                            break\n",
    "                            \n",
    "                        #check for AL- or W- \n",
    "                        \n",
    "                    else: \n",
    "\n",
    "                        stemmed_placename , segmes = drop_prefix(n_gram_clean_token)[0] , drop_prefix(n_gram_clean_token)[1] \n",
    "\n",
    "                        if stemmed_placename in ngrams_placenames[str(ix)] and not pre_word and stemmed_placename != placename :\n",
    "                        \n",
    "                            if(segmes[0] in ['ب','بال']):\n",
    "\n",
    "\n",
    "                                _dict.append({'prep':segmes[0],'@index':str(index - 0.5)})\n",
    "                                _dict.append({'placename':stemmed_placename,'@index':index})\n",
    "                                skip_index=ix\n",
    "                                flag = False \n",
    "                                break\n",
    "                                if debug:\n",
    "                                    print(_dict)\n",
    "\n",
    "                               \n",
    "                            else:\n",
    "                                _dict.append({'placename':stemmed_placename,'@index':index})\n",
    "                                skip_index=ix\n",
    "                                flag = False \n",
    "                                break\n",
    "\n",
    "                                    \n",
    "\n",
    "                            \n",
    "            if flag :\n",
    "                \n",
    "                _pre_drop = drop_prefix(tokens[index])[0]\n",
    "                if tokens[index] in types:\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    _dict.append({'type':stem_tokens[index],'@index':index})\n",
    "                    pre_word = False\n",
    "                    \n",
    "                elif _pre_drop in types:\n",
    "                    \n",
    "  \n",
    "                    \n",
    "                    _dict.append({'type': _pre_drop,'@index':index})\n",
    "                    pre_word = False\n",
    "\n",
    "\n",
    "        \n",
    "                if stem_tokens[index] in preps:\n",
    "                    # TODO:\n",
    "                    \n",
    "                    \n",
    "                    _dict.append({'prep':tokens[index],'@index':index})\n",
    "                    \n",
    "                elif stem_tokens[index] == 'بين':\n",
    "                    pre_word = True\n",
    "                else:\n",
    "                    pre_word = False\n",
    "\n",
    "               \n",
    "        if _dict:\n",
    "#             print(_dict)\n",
    "#             print(parser_and_grep(_dict))\n",
    "            if len(_dict) >= 3:\n",
    "                    \n",
    "                if _filter :\n",
    "                    \n",
    "                    if parser_and_grep(_dict):\n",
    "                        \n",
    "                        entities.append(_dict) \n",
    "                else:\n",
    "                    if not parser_and_grep(_dict):\n",
    "                        entities.append(_dict) \n",
    "#         print(sentence,_dict)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_false ['Filterd_rule'] = rules_false[['PlaceName','Description']].apply(lambda x : rule_based5(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biwords = rules_false[rules_false ['Filterd_rule'].map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biwords['rule_text'] = rules_false['Filterd_rule'].apply(lambda x : parse_rule(x))\n",
    "df_biwords['id'] = df_biwords.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_false[rules_false['id'] == 8291]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rule_based4([rules_false.iloc[2000].PlaceName,rules_false.iloc[2000].Description],_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdb.Connector().update_rule_table(df_biwords[['id','PlaceName','Filterd_rule','rule_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dublicate = yaqutz_pd.iloc[8291][['PlaceName','Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dict = []\n",
    "result= rule_based(clean_ex.split(),stem_ex.split(),0,_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  input : list of dict \n",
    "[{'type': 'قرية', '@index': 9},\n",
    " {'prep': 'من', '@index': 10},\n",
    " {'type': 'قرى', '@index': 11},\n",
    " {'placename': 'حزم نمير', '@index': 12}]\n",
    " \n",
    "    return: True if the rule is valid \n",
    " '''\n",
    "parser_and_grep([{'prep': 'من', '@index': 0},\n",
    "  {'prep': 'من', '@index': 6},\n",
    "  {'type': 'ما', '@index': 11},\n",
    "  {'prep': 'فيها', '@index': 14},\n",
    "  {'prep': 'من', '@index': 15},\n",
    "  {'placename': 'ملك', '@index': 16},\n",
    "  {'placename': 'عمران', '@index': 17},\n",
    "  {'placename': 'ومزاهر', '@index': 18},\n",
    "  {'placename': 'وعرمان', '@index': 19},\n",
    "  {'placename': 'وملح', '@index': 20},\n",
    "  {'placename': 'ومحجر', '@index': 21},\n",
    "  {'type': 'ما', '@index': 22},\n",
    "  {'prep': 'من', '@index': 25},\n",
    "  {'placename': 'ييعث', '@index': 28},\n",
    "  {'type': 'ما', '@index': 30},\n",
    "  {'prep': 'من', '@index': 33}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prefix('بالبصرة')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(2,-1,-1):\n",
    "    print(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1','2','3','4']\n",
    "for i in range(4):\n",
    "    for ix in range(3,0,-1):\n",
    "        if(i+ix < 5 ):\n",
    "            print(x[i:i+ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 'tpptprrp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Special case of ARAN And Atharbjan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aran_disc =\"\"\"وهي بلدة كبيرة بما وراء النهر من بلاد\n",
    "<br />~~الهياطلة بين سيحون وسمرقند، وبينها وبين سمرقند ستة وعشرون فرسخا، معدودة\n",
    "<br />~~في الإقليم الرابع،\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rule_based3(aran_disc,debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num in arabic_numbers:\n",
    "    df = yaqutz_pd.loc[yaqutz_pd['PlaceName'] == num]\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yaqutz_pd.loc[yaqutz_pd['PlaceName'] == 'دهناء']\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'عواصم' in placenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prefix('أكراد')[0] in placenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentor.segment('وأران')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='منها جنزة ، وهي التي تسميها العامة كنجة ، وبرذعة ، وشمكور ، وبيلقان . وبين'\n",
    "x = text.replace(\".\", \"d\")\n",
    "print(x)\n",
    "pattern = re.compile(r\" d \")\n",
    "print(pattern.split(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aran_disc = '''# بالفتح، ثم السكون، وفتح الراء، وكسر الباء الموحدة، وياء ساكنة، وجيم،\n",
    "<br />~~هكذا جاء في شعر الشماخ:\n",
    "<br /># تذكرتها وهنا، وقد حال دونها % % قرى أذربيجان المسالح والجال\n",
    "<br /># وقد فتح قوم الذال، وسكنوا الراء، ومد آخرون الهمزة مع ذلك. وروي عن\n",
    "<br />~~المهلب، ولا أعرف المهلب هذا، آذربيجان، بمد الهمزة، وسكون الذال، فيلتقي\n",
    "<br />~~ساكنان، وكسر الراء، ثم ياء ساكنة، وباء موحدة مفتوحة، وجيم، وألف، ونون.\n",
    "<br /># قال أبو عون إسحاق بن علي في زيحه: أذربيجان في الإقليم الخامس، طولها\n",
    "<br />~~ثلاث وسبعون درجة، وعرضها أربعون درجة. قال النحويون: النسبة إليه أذري،\n",
    "<br />~~بالتحريك، وقيل: أذري بسكون الذال، لأنه عندهم مركب من أذر وبيجان، فالنسبة\n",
    "<br />~~إلى الشطر الأول، وقيل أذربي، كل قد جاء. وهو اسم اجتمعت فيه خمس موانع من\n",
    "<br />~~الصرف: العجمة، والتعريف، والتأنيث، والتركيب، ولحاق الألف والنون، ومع\n",
    "<br />~~ذلك، فانه إذا زالت عنه إحدى هذه الموانع، وهو التعريف، صرف، لأن هذه\n",
    "<br />~~الأسباب لا تكون موانع من الصرف، إلا مع العلمية، فإذا زالت العلمية بطل\n",
    "<br />~~حكم البواقي، ولولا ذلك، لكان مثل قائمة، ومانعة، ومطيعة، غير منصرف، لأن\n",
    "<br />~~فيه التأنيث، والوصف، ولكان مثل الفرند، واللجام، غير منصرف لاجتماع\n",
    "<br />~~العجمة والوصف فيه، وكذلك الكتمان، لأن فيه Milestone300 الألف والنون، والوصف، فاعرف\n",
    "<br />~~ذلك. قال ابن المقفع: أذربيجان مسماة باذرباذ بن إيران بن الأسود بن سام\n",
    "<br />~~بن نوح، عليه السلام، وقيل:\n",
    "<br /># أذرباذ بن بيوراسف، وقيل: بل أذر اسم النار بالفهلوية، وبايكان معناه\n",
    "<br />~~الحافظ والخازن، فكأن معناه بيت النار، أو خازن النار، وهذا أشبه بالحق\n",
    "<br />~~وأحرى به، لأن بيوت النار في هذه الناحية كانت كثيرة جدا. وحد أذربيجان\n",
    "<br />~~من برذعة مشرقا إلى أرزنجان مغربا، ويتصل حدها من جهة الشمال ببلاد\n",
    "<br />~~الديلم، والجيل، والطرم، وهو إقليم واسع. ومن مشهور مدائنها: تبريز، وهي\n",
    "<br />~~اليوم قصبتها وأكبر مدنها، وكانت قصبتها قديما المراغة، ومن مدنها خوي،\n",
    "<br />~~وسلماس، وأرمية، وأردبيل، ومرند، وغير ذلك. وهو صقع جليل، ومملكة عظيمة،\n",
    "<br />~~الغالب عليها الجبال، وفيه قلاع كثيرة، وخيرات واسعة، وفواكه جمة، ما رأيت\n",
    "<br />~~ناحية أكثر بساتين منها، ولا أغزر مياها وعيونا، لا يحتاج السائر بنواحيها\n",
    "<br />~~إلى حمل إناء للماء، لأن المياه جارية تحت أقدامه أين توجه، وهو ماء بارد\n",
    "<br />~~عذب صحيح. وأهلها صباح الوجوه حمرها، رقاق البشرة، ولهم لغة يقال لها:\n",
    "<br />~~الأذرية، لا يفهمها غيرهم. وفي أهلها لين وحسن معاملة، إلا أن البخل يغلب\n",
    "<br />~~على طباعهم. وهي بلاد فتنة وحروب، ما خلت قط منها، فلذلك أكثر مدنها خراب،\n",
    "<br />~~وقراها يباب. وفي أيامنا هذه، هي مملكة PageV01P128\n",
    "<br /># جلال الدين منكبرنى بن علاء الدين محمد بن تكش خوارزم شاه. وقد فتحت أولا\n",
    "<br />~~في أيام عمر بن الخطاب، رضي الله عنه، وكان عمر قد أنفذ المغيرة بن شعبة\n",
    "<br />~~الثقفي واليا على الكوفة، ومعه كتاب إلى حذيفة بن اليمان، بولاية\n",
    "<br />~~أذربيجان، فورد الكتاب على حذيفة وهو بنهاوند، فسار منها إلى أذربيجان في\n",
    "<br />~~جيش كثيف، حتى أتى أردبيل، وهي يومئذ مدينة أذربيجان. وكان مرزبانها قد جمع\n",
    "<br />~~المقاتلة من أهل باجروان، وميمذ، والبذ، وسراو، وشيز، والميانج، وغيرها،\n",
    "<br />~~فقاتلوا المسلمين قتالا شديدا أياما. ثم إن المرزبان صالح حذيفة على جميع\n",
    "<br />~~أذربيجان، على ثمانمائة ألف درهم وزن، على أن لا يقتل منهم أحدا، ولا\n",
    "<br />~~يسبيه، ولا يهدم بيت نار، ولا يعرض لأكراد البلاشجان، وسبلان، وميان روذان،\n",
    "<br />~~ولا Milestone300 يمنع أهل الشيز خاصة من الزفن في أعيادهم، وإظهار ما كانوا يظهرونه.\n",
    "<br />~~ثم إنه غزا موقان، وجيلان، فأوقع بهم، وصالحهم على إتاوة. ثم إن عمر، رضي\n",
    "<br />~~الله عنه، عزل حذيفة، وولى عتبة بن فرقد على أذربيجان، فأتاها من الموصل،\n",
    "<br />~~ويقال: بل أتاها من شهرزور على السلق الذي يعرف بمعاوية الأذري، فلما دخل\n",
    "<br />~~أردبيل، وجد أهلها على العهد، وقد انتقضت عليه نواح، فغزاها وظفر وغنم،\n",
    "<br />~~فكان معه ابنه عمرو بن عتبة بن فرقد الزاهد، وعن الواقدي:\n",
    "<br /># غزا المغيرة بن شعبة أذربيجان من الكوفة، سنة اثنتين وعشرين، ففتحها\n",
    "<br />~~عنوة، ووضع عليها الخراج.\n",
    "<br /># وروى أبو المنذر هشام بن محمد عن أبي مخنف، أن المغيرة بن شعبة غزا\n",
    "<br />~~أذربيجان في سنة عشرين ففتحها، ثم إنهم كفروا، فغزاهم الأشعث بن قيس\n",
    "<br />~~الكندي، ففتح حصن جابروان، وصالحهم على صلح المغيرة، ومضى صلح الأشعث إلى\n",
    "<br />~~اليوم. وقال المدائني:\n",
    "<br /># لما هزم المشركون بنهاوند، رجع الناس إلى أمصارهم، وبقي أهل الكوفة مع\n",
    "<br />~~حذيفة، فغزا بهم أذربيجان، فصالحهم على ثمانمائة ألف درهم، ولما استعمل\n",
    "<br />~~عثمان بن عفان، رضي الله عنه، الوليد بن عقبة على الكوفة، عزل عتبة بن فرقد\n",
    "<br />~~عن أذربيجان، فنقضوا، فغزاهم الوليد بن عقبة سنة خمس وعشرين، وعلى مقدمته\n",
    "<br />~~عبد الله بن شبيل الأحمسي، فأغار على أهل موقان، والتبريز، والطيلسان،\n",
    "<br />~~فغنم وسبا، ثم صالح أهل أذربيجان على صلح حذيفة.\n",
    "'''\n",
    "rule_based3(aran_disc,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _s(place):\n",
    "    print(\" \".join(s for s in list(placenames) if place in s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_s('بكر')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"# بفتح الهمزة وسكون الألف وفتح الباء الموحدة والسين Milestone300 المهملة ساكنة وكاف\n",
    "<br />~~مضمومة وواو ساكنة ونون، ورواه بعضهم بهمزة بعدها باء ليس بينهما ألف وقد\n",
    "<br />~~ذكر في موضعه: بليدة على ساحل بحر طبرستان بينها وبين جرجان ثلاثة أيام،\n",
    "<br />~~وإليها ينسب بحر آبسكون، وينسب إليها أبو العلاء احمد بن صالح بن محمد بن\n",
    "<br />~~صالح التميمي الآبسكوني، كان ينزل بصور على ساحل بحر الشام.\n",
    "<br /># 4 معجم البلدان دار صادر PageV01P049\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_based3(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys = [(p + ' بلاد ').strip(' ') for p in preps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in querys:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [ str(i) for i in range(10)]\n",
    "print('0' in ['9','0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-Al - usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Place names form Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'تعكر' in ngrams_placenames['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stword = pd.read_csv('data/dumb/bi_grams_1stWord.csv')\n",
    "stword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stword = stword.head(118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stword = set(stword['name'].apply(lambda x : drop_prefix(x)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stword.add('أبي')\n",
    "'أبي' in stword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bi_grams_boldan(text,_word):\n",
    "    text = clean_text(sentence=text,hard=True,stem=False).split()\n",
    "    _dict = []\n",
    "    for word in range(len(text)-3):\n",
    "        if text[word] in preps and text[word+1] ==_word:\n",
    "            phrase = text[word] +' ' + text[word+1] + ' ' +  text[word+2]\n",
    "            \n",
    "            if text[word+2] in stword : \n",
    "                phrase+= ' ' + text[word+3]\n",
    "            placename = ' '.join(phrase.split()[2:])\n",
    "            _len = str(len(phrase.split()[2:]))\n",
    "            \n",
    "            if(placename not in ngrams_placenames[_len]):\n",
    "                _dict.append(phrase)\n",
    "    return _dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bi_grams_known(text,_word):\n",
    "    text = clean_text(sentence=text,hard=True,stem=False).split()\n",
    "    _dict = []\n",
    "    for word in range(len(text)-3):\n",
    "        if text[word] in preps and text[word+1] ==word:\n",
    "            phrase = text[word] +' ' + text[word+1] + ' ' + text[word+2]\n",
    "            \n",
    "            \n",
    "            if drop_prefix(text[word+2]) +' ' +  drop_prefix(text[word+3]) in ngrams_placenames['2']:\n",
    "                print()\n",
    "                _dict.append(drop_prefix(text[word+2]) +' ' +  drop_prefix(text[word+3]))\n",
    "                \n",
    "            elif drop_prefix(text[word+2]) in ngrams_placenames['1']:\n",
    "            \n",
    "                _dict.append(text[word+2])\n",
    "    return (_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = yaqutz_pd.Description.apply(lambda x : match_bi_grams_boldan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff[dff.map(lambda d: len(d)) > 0].to_csv('fe_belad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = dff[dff.map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=[]\n",
    "for i in dff.values:\n",
    "    print(i)\n",
    "    for j in i:\n",
    "        d.append(j)\n",
    "Placenames_candidates = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Placenames_candidates = [' '.join(x.split()[2:]) for x in Placenames_candidates]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Placenames_candidates.to_csv('data/util/Placenames_inferd_from_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Placenames_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Placenames_candidates).to_csv('data/util/Placenames_inferd_from_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'جزيرة' in ngrams_placenames['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = yaqutz_pd.Description.apply(lambda x : match_bi_grams_known_boldan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2.map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyar_df = yaqutz_pd.Description.apply(lambda x : match_bi_grams_boldan(x,'ديار'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dyar_df = dyar_df[dyar_df.map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= ''' موضع في ديار بلقين بن جسر'''\n",
    "match_bi_grams_boldan(t,'ديار')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
